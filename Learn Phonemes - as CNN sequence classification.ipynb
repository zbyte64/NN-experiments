{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arpabet = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EH1', 'S']]\n",
      "[['S', 'IY1']]\n",
      "[['S', 'IY1']]\n",
      "[['K', 'AH0', 'M', 'P', 'Y', 'UW1', 'T']]\n",
      "'comput'\n",
      "'seesea'\n"
     ]
    }
   ],
   "source": [
    "for word in ('s', 'see', 'sea', 'compute', 'comput', 'seesea'):\n",
    "    try:\n",
    "        print(arpabet[word])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arpabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing\n",
    "\n",
    "* Normalize the record sizes\n",
    "* Do not one-hot encode, leave that for the neural net\n",
    "* Do label encoding to numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "class LabelEncoder(object):\n",
    "    '''\n",
    "    A progressive label encoder\n",
    "    '''\n",
    "    def __init__(self, dim=1):\n",
    "        self.dim = dim\n",
    "        self.classes_ = dict()\n",
    "        self.lookup_ = dict()\n",
    "        self.max_sequence_size = 0\n",
    "    \n",
    "    @property\n",
    "    def n_classes(self):\n",
    "        return len(self.classes_)\n",
    "    \n",
    "    def _all_labels(self, data, update_size=False):\n",
    "        #TODO use self.dim\n",
    "        for seq in data:\n",
    "            if update_size:\n",
    "                self.max_sequence_size = max(self.max_sequence_size, len(seq))\n",
    "            for label in seq:\n",
    "                yield label\n",
    "    \n",
    "    def fit(self, data):\n",
    "        for t in map(self.tokenize_label, self._all_labels(data, update_size=True)):\n",
    "            pass\n",
    "        #self.onehot_encoder = sklearn.preprocessing.OneHotEncoder(self.n_classes)\n",
    "    \n",
    "    def refit(self, data):\n",
    "        batch_size = len(data)\n",
    "        max_size = self.max_sequence_size\n",
    "        o_encoded = np.empty((batch_size, max_size), dtype='object')\n",
    "        o_encoded.fill(self.lookup_[0])\n",
    "\n",
    "        for i, seq in enumerate(data):\n",
    "            o_encoded[i,0:len(seq)] = seq\n",
    "        return o_encoded\n",
    "    \n",
    "    def transform(self, data):\n",
    "        batch_size = len(data)\n",
    "        max_size = self.max_sequence_size\n",
    "        o_encoded = np.zeros((batch_size, max_size), dtype='int32')\n",
    "\n",
    "        for i, seq in enumerate(data):\n",
    "            for j, syl in enumerate(seq):\n",
    "                o_encoded[i,j] = self.tokenize_label(syl)\n",
    "        return o_encoded\n",
    "    \n",
    "    def inverse_transform(self, data):\n",
    "        batch_size = len(data)\n",
    "        max_size = self.max_sequence_size\n",
    "        decoded = np.zeros((batch_size, max_size), dtype='object')\n",
    "\n",
    "        for i, seq in enumerate(data):\n",
    "            for j, index in enumerate(seq):\n",
    "                decoded[i,j] = self.lookup_[index]\n",
    "        return decoded\n",
    "        \n",
    "    def onehot_label(self, label):\n",
    "        token = self.tokenize_label(label)\n",
    "        return self.onehot_encoder.fit_transform(token).toarray()\n",
    "    \n",
    "    def tokenize_label(self, label):\n",
    "        if label in self.classes_:\n",
    "            return self.classes_[label]\n",
    "        else:\n",
    "            index = len(self.classes_)\n",
    "            self.classes_[label] = index\n",
    "            self.lookup_[index] = label\n",
    "            return index\n",
    "\n",
    "\n",
    "def track_and_refit(data, null_class=''):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit([[null_class]])\n",
    "    encoder.fit(data)\n",
    "    return encoder.transform(data), encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'a', 'p', 'k', 'e'] ['HH', 'EY1', 'P', 'K', 'IY0']\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "WARNING:tensorflow:The default value of combiner will change from \"sum\" to \"sqrtn\" after 2016/11/01.\n",
      "133737 133737\n",
      "[1 2 3 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 2 3 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X_, y_ = [], []\n",
    "\n",
    "for word, utterances in arpabet.items():\n",
    "    X_.extend([list(word)]*len(utterances))\n",
    "    y_.extend(utterances)\n",
    "\n",
    "    \n",
    "print(X_[0], y_[0])\n",
    "\n",
    "X, X_enc = track_and_refit(X_)\n",
    "X_classes = list(X_enc.classes_.keys())\n",
    "y, y_enc = track_and_refit(y_)\n",
    "y_classes = list(y_enc.classes_.keys())\n",
    "\n",
    "col_chars = [tf.contrib.layers.sparse_column_with_keys(column_name=\"char\"+str(i), keys=X_classes)\n",
    "            for i in range(X_enc.max_sequence_size)]\n",
    "col_seq = [tf.contrib.layers.sparse_column_with_keys(column_name=\"seq\"+str(i), keys=y_classes)\n",
    "            for i in range(y_enc.max_sequence_size)]\n",
    "\n",
    "print(len(X), len(y))\n",
    "print(X[0])\n",
    "print(y[0])\n",
    "n_classes = y_enc.n_classes\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y)#, stratify=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpdk7qfwj5\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_task_id': 0, '_num_ps_replicas': 0, '_master': '', '_environment': 'local', 'save_summary_steps': 100, 'keep_checkpoint_max': 5, 'save_checkpoints_secs': 600, '_evaluation_master': '', 'tf_random_seed': None, 'save_checkpoints_steps': None, 'keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2227f78128>}\n",
      "Model: (?, 33) -> (?, 32)\n",
      "55 71\n",
      "Onehot: (?, 33, 55) -> (?, 32, 71)\n",
      "Features: (?, 33, 55)\n",
      "Features: (?, 32, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "S: (?, 1, 71) -> (?, 1, 71)\n",
      "losses: (32,)\n",
      "loss: ()\n",
      "p[0] <unknown>\n",
      "prediction: <unknown>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6385ec165c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSKCompat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO undo onhot?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jason/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, steps, max_steps, monitors)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0mmonitors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         max_steps=max_steps)\n\u001b[0m\u001b[1;32m   1132\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jason/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\u001b[0m\n\u001b[1;32m    697\u001b[0m       \u001b[0;31m# cases, but will soon be deleted after the subclasses are updated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m       \u001b[0;31m# TODO(b/32664904): Update subclasses and delete the else-statement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m       \u001b[0mtrain_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jason/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_get_train_ops\u001b[0;34m(self, features, labels)\u001b[0m\n\u001b[1;32m   1050\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \"\"\"\n\u001b[0;32m-> 1052\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_eval_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jason/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m       \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-6385ec165c60>\u001b[0m in \u001b[0;36mmy_model\u001b[0;34m(features, target)\u001b[0m\n\u001b[1;32m     70\u001b[0m     train_op = tf.contrib.layers.optimize_loss(\n\u001b[1;32m     71\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adagrad'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         learning_rate=0.01)\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;31m#return {'class': tf.argmax(prediction, 1), 'prob': prediction}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jason/.local/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\u001b[0m in \u001b[0;36moptimize_loss\u001b[0;34m(loss, global_step, learning_rate, optimizer, gradient_noise_scale, gradient_multipliers, clip_gradients, learning_rate_decay_fn, update_ops, variables, name, summaries, colocate_gradients_with_ops)\u001b[0m\n\u001b[1;32m    273\u001b[0m     grad_updates = opt.apply_gradients(gradients,\n\u001b[1;32m    274\u001b[0m                                        \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                                        name=\"train\")\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;31m# Ensure the train_tensor computes grad_updates.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jason/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m       raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m--> 391\u001b[0;31m                        ([str(v) for _, v in converted_grads_and_vars],))\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jason/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m       raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m--> 391\u001b[0;31m                        ([str(v) for _, v in converted_grads_and_vars],))\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, metrics, cross_validation\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "def my_model(features, target):\n",
    "    print('Model:', features.get_shape(), '->', target.get_shape())\n",
    "    \n",
    "    feature_classes = X_enc.n_classes\n",
    "    target_classes = y_enc.n_classes\n",
    "    sequence_size = y_enc.max_sequence_size\n",
    "    print(feature_classes, target_classes)\n",
    "    target = tf.one_hot(target, target_classes, 1, 0)\n",
    "    features = tf.to_float(tf.one_hot(features, feature_classes, 1, 0))\n",
    "    print('Onehot:', features.get_shape(), '->', target.get_shape())\n",
    "    \n",
    "    conv_filter = tf.Variable(tf.zeros([5, feature_classes, target_classes]))\n",
    "    layer = tf.nn.conv1d(features, conv_filter, stride=1, padding='SAME')\n",
    "    #features = tf.tanh(features)\n",
    "    #print('Conv1', features.get_shape())\n",
    "    \n",
    "    #features = layers.stack(features, layers.fully_connected, [target_classes, target_classes])\n",
    "    print('Features:', features.get_shape())\n",
    "    #features = tf.nn.dropout(features, .3)\n",
    "    #features = layers.fully_connected(features, target_classes, activation_fn=tf.tanh)#TODO fully connected tanh + dropout\n",
    "    features = tf.slice(features, [0,0,0], [-1, sequence_size, target_classes])\n",
    "    \n",
    "    print('Features:', features.get_shape())\n",
    "    \n",
    "    #Use `tf.contrib.losses.softmax_cross_entropy` and explicit logits computation.\n",
    "    # Compute logits (1 per class) and compute loss.\n",
    "    \n",
    "    #set up sequence to sequence loss function\n",
    "    multi_feats = tf.split(1, sequence_size, features)\n",
    "    multi_targets = tf.split(1, sequence_size, target)\n",
    "    losses = list()\n",
    "    predictions = list()\n",
    "    for seq_target, seq_feats in zip(multi_targets, multi_feats):\n",
    "        #seq_feats = tf.squeeze(seq_feats)\n",
    "        print('S:', seq_feats.get_shape(), '->', seq_target.get_shape())\n",
    "        seq_loss = tf.contrib.losses.softmax_cross_entropy(seq_feats, seq_target)\n",
    "        seq_pred = tf.squeeze(tf.argmax(seq_feats, 2))\n",
    "        losses.append(seq_loss)\n",
    "        predictions.append(seq_pred)\n",
    "    losses = tf.pack(losses)\n",
    "    #or\n",
    "    #losses = tf.nn.ctc_loss(features, target, sequence_size, time_major=False)#, preprocess_collapse_repeated=False, ctc_merge_repeated=True, time_major=True)\n",
    "    print('losses:', losses.get_shape())\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    prediction = tf.pack(predictions, axis=1)\n",
    "    \n",
    "    \n",
    "    print('loss:', loss.get_shape())\n",
    "    #print('predictions:', predictions)\n",
    "    print('p[0]', predictions[0].get_shape())\n",
    "    print('prediction:', prediction.get_shape())\n",
    "    \n",
    "    '''\n",
    "    logits = layers.fully_connected(features, target_classes) #softmax\n",
    "    #loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "    \n",
    "    #loss = tf.nn.softmax_cross_entropy_with_logits(logits, target)\n",
    "    print('logits/labels:', logits.get_shape(), target.get_shape())\n",
    "    '''\n",
    "    \n",
    "    #prediction, loss = (\n",
    "    #    tf.contrib.learn.models.logistic_regression_zero_init(features, target)\n",
    "    #)\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "        loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n",
    "        learning_rate=0.01)\n",
    "    #return {'class': tf.argmax(prediction, 1), 'prob': prediction}\n",
    "    return prediction, loss, train_op\n",
    "\n",
    "\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn=my_model))\n",
    "classifier.fit(X_train, y_train, steps=100000)\n",
    "\n",
    "predictions = list(classifier.predict(X_test)) #TODO undo onhot?\n",
    "#print(\"Accuracy:\", metrics.accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"Accuracy:\", metrics.accuracy_score(y_test, predictions))\n",
    "#X_enc.inverse_transform(X_test[:5])\n",
    "#y_enc.inverse_transform(predictions[:5])\n",
    "print(X_test[0], predictions[0])\n",
    "print(X_enc.inverse_transform([X_test[0]]))\n",
    "print(y_enc.inverse_transform([predictions[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
